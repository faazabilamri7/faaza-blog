---
title: What Are Network Effects in A/B Testing?
category: data-scientist
tags:
  - data-scientist
  - 2-statistics
  - ab-testing
description: "Hey there.  Imagine you're running a lemonade stand and want to know if adding\na new sign will boost sales."
pubDate: '2025-10-08T22:03:46.212Z'
draft: false
excerpt: "Hey there.  Imagine you're running a lemonade stand and want to know if adding\na new sign will boost sales."
---

Hey there! Imagine you're running a lemonade stand and want to know if adding a new sign will boost sales. You test it by showing the sign to half your customers and not to the other half, then compare sales. That's basically A/B testing—a simple way to compare two versions and see which works better. But what if your customers talk to each other and influence each other's decisions? This "network effect" can mess up your results, making it hard to know if the sign really helped or if word-of-mouth did the trick.

In this article, we'll explore how network effects complicate A/B testing, using real examples from companies like DoorDash. We'll break it down into simple terms, explain why it's a problem, and share practical solutions. By the end, you'll understand how to design better experiments that account for these sneaky influences. (This is based on DoorDash's insights from [their blog post on CUPAC](https://careersatdoordash.com/blog/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/).)

## What Are Network Effects in A/B Testing?

Network effects happen when users influence each other. In A/B testing, this means a change in one group might affect the other group through social connections, making it tough to isolate the true impact of your test.

Think of it like this: You're testing a new recipe at a potluck. Group A gets the new recipe, Group B gets the old one. But if people from Group A rave about it to Group B, some in Group B might try the new recipe too. Now, your results are skewed—did Group B's improvement come from the recipe or from the chatter?

In online experiments, this could be friends sharing a new feature on social media or competitors reacting to a price change. DoorDash deals with this in food delivery, where user behavior (like ordering habits) can spread through networks.

## Why Network Effects Are a Big Problem

Network effects create "spillover"—when the treatment group's actions leak into the control group. This makes your test less reliable because you can't tell if differences are due to your change or external influences.

For example, if you're testing a new app feature that makes users more engaged, they might tell their friends, who then use the app more—even if they're in the control group. Your test might show both groups improving, underestimating the feature's true power.

DoorDash's research shows this can lead to biased results, especially in markets with strong social ties. Without fixing it, you might launch a bad idea or miss a good one.

## Solutions to Handle Network Effects

The good news? There are ways to minimize network effects. DoorDash uses advanced techniques like CUPAC (Controlled experiments Utilizing Pre-experiment data as Covariates) to adjust for these influences.

### 1. **Geographic Randomization**
Assign entire regions (like cities or neighborhoods) to treatment or control groups instead of individual users. This prevents spillover within the same area.

- **Pros**: Completely isolates groups, reducing network bias.
- **Cons**: Different regions might have natural differences (e.g., urban vs. rural), so you need to account for that in analysis.
- **When to use**: For features that spread locally, like local promotions.

### 2. **Time-Based Randomization**
Run the test in different time periods. For example, treatment on Mondays and Wednesdays, control on Tuesdays and Thursdays.

- **Pros**: Easy to implement and avoids ongoing interference.
- **Cons**: Doesn't work for long-term effects or if behavior changes over time.
- **When to use**: For short-term tests like ad campaigns.

### 3. **Network Cluster Randomization**
Group users by their social connections (e.g., friend networks) and assign entire clusters to the same group.

- **Pros**: Keeps influences within groups, preserving natural behavior.
- **Cons**: Hard to identify clusters accurately without detailed data.
- **When to use**: In social apps or platforms with strong user networks.

### 4. **CUPAC: Adjusting with Pre-Experiment Data**
CUPAC uses data from before the test (like past user behavior) to predict and subtract network influences. It's like correcting for background noise in your results.

- **How it works**: Build a model predicting outcomes based on pre-test data, then adjust test results to remove predicted effects.
- **Pros**: Works with existing data, boosts test power without more users.
- **Cons**: Needs good pre-data; not perfect for all scenarios.
- **When to use**: When you have historical user data, as DoorDash does.

DoorDash found CUPAC cut experiment times by 25% while keeping accuracy, proving it's a game-changer for noisy data.

## Wrapping It Up: Key Takeaways

Network effects can turn A/B testing into a guessing game, but with smart strategies like geographic or time-based randomization and tools like CUPAC, you can get clearer results. Remember:
- Identify potential spillover early.
- Choose the right isolation method for your context.
- Use pre-experiment data to adjust for biases.

By accounting for these effects, you'll make more confident decisions, just like DoorDash does.

## Quick FAQ for Beginners

**Q: How do I know if network effects are affecting my test?**
A: Look for unexpected similarities between groups or results that don't match your expectations. Run a pilot test to check.

**Q: Can small businesses use these techniques?**
A: Yes! Start with time-based randomization—it's simple and effective without fancy tools.

**Q: What's the easiest way to start?**
A: Use CUPAC if you have past data; otherwise, try geographic splits for local tests.

Ready to test? Try a small A/B test on your website and watch for network influences. Share your experiences in the comments—what's your biggest testing challenge?

---
title: '---'
category: general
tags:
  - general
  - auto-post
description: "title: 'Building Docker Images: Creating Custom Containers for Your Applications'\ndescription: 'Master the art of building Docker images using Docker..."
pubDate: '2025-10-09T02:10:36.671Z'
draft: false
excerpt: "title: 'Building Docker Images: Creating Custom Containers for Your Applications'\ndescription: 'Master the art of building Docker images using Docker..."
---

title: 'Building Docker Images: Creating Custom Containers for Your Applications'
description: 'Master the art of building Docker images using Dockerfiles. Learn about image layers, best practices, container registries, and how to create efficient, production-ready container images.'
publishDate: 2025-10-09
tags: ['docker', 'dockerfile', 'docker-images', 'container-registry', 'devops']
draft: false
---

## Introduction

You've been running containers from pre-built images on Docker Hub, and that's great! But the real power of Docker comes when you can package _your own_ applications into containers.

Imagine you've built a web application. Maybe it's a Node.js API, a Python Flask app, or a Go microservice. How do you turn that application into a Docker container that you can run anywhere? The answer is: you build a Docker image.

In this comprehensive guide, you'll learn how to write Dockerfiles (the recipe for building images), understand how Docker builds images layer by layer, follow best practices to create efficient images, and publish your images to container registries so they can be shared and deployed.

By the end, you'll be able to take any application you've written and containerize it like a pro.

**Source:** [Complete Docker Course - From BEGINNER to PRO!](https://www.youtube.com/watch?v=RqTEHSBrYFw) (1:13:00 - 3:02:36)

## Understanding Docker Images: The Blueprint for Containers

Before we start building, let's make sure we understand what a Docker image actually is.

### What is a Docker Image?

Think of a Docker image like a **recipe** or a **blueprint**:

- A **recipe** tells you how to make a cake (the ingredients and steps)
- A **blueprint** shows how to build a house (the materials and structure)
- A **Docker image** defines how to create a container (the files, dependencies, and configuration)

### The Layered Architecture

Remember from our first article: Docker images are made of **layers**, like a stack of pancakes:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 4: Your app code ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 3: Dependencies  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 2: Runtime (e.g. Node.js) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 1: Base OS (e.g. Alpine)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Why layers matter:**

1. **Efficiency:** Layers are cached. If you change your app code (layer 4), Docker only rebuilds that layer‚Äînot the entire image.
2. **Sharing:** Multiple images can share base layers, saving disk space.
3. **Speed:** Downloading only changed layers makes transfers faster.

Each instruction in your Dockerfile typically creates a new layer.

## Your First Dockerfile: A Simple Node.js App

Let's start with a practical example. We'll build a simple Node.js application into a Docker image.

### The Application

Here's a minimal Node.js web server (`app.js`):

```javascript
const express = require('express');
const app = express();
const PORT = 3000;

app.get('/', (req, res) => {
  res.send('Hello from Docker!');
});

app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

And the `package.json`:

```json
{
  "name": "docker-demo",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2"
  },
  "scripts": {
    "start": "node app.js"
  }
}
```

### The Dockerfile

Create a file named `Dockerfile` (no extension!) in the same directory:

```dockerfile
# Start with a base image
FROM node:18-alpine

# Set the working directory inside the container
WORKDIR /app

# Copy package.json and package-lock.json
COPY package*.json ./

# Install dependencies
RUN npm install

# Copy the rest of the application code
COPY . .

# Expose the port the app runs on
EXPOSE 3000

# Define the command to run the app
CMD ["npm", "start"]
```

### Breaking Down Each Instruction

Let's understand what each line does:

#### `FROM node:18-alpine`

**What it does:** Specifies the base image to build from.

**Why it matters:** Instead of starting from scratch, we start with an image that already has Node.js 18 installed on Alpine Linux.

**Think of it like:** You're building a house. Instead of pouring a foundation, you're starting with a pre-built foundation.

#### `WORKDIR /app`

**What it does:** Sets the working directory inside the container to `/app`.

**Why it matters:** All subsequent commands will run in this directory. If the directory doesn't exist, Docker creates it.

**Think of it like:** Choosing which folder to work in on your computer.

#### `COPY package*.json ./`

**What it does:** Copies `package.json` and `package-lock.json` from your computer into the `/app` directory in the container.

**Why we do this separately:** So Docker can cache the dependency installation layer. If package files don't change, Docker skips reinstalling dependencies.

**The `./`** means "current directory" (which is `/app` because of `WORKDIR`).

#### `RUN npm install`

**What it does:** Runs the command `npm install` inside the container.

**Why it matters:** This installs all the Node.js dependencies specified in `package.json`.

**Important:** `RUN` commands execute during the _build_ process, not when the container runs.

#### `COPY . .`

**What it does:** Copies all remaining files from your project directory into `/app`.

**Why we do this after dependencies:** Because source code changes more frequently than dependencies. By copying source code last, we maximize cache hits.

#### `EXPOSE 3000`

**What it does:** Documents that the container listens on port 3000.

**Important clarification:** This does NOT automatically publish the port! It's just documentation. You still need `-p 3000:3000` when running the container.

**Think of it like:** A label on a box saying "fragile"‚Äîit's informative, but doesn't change how the box works.

#### `CMD ["npm", "start"]`

**What it does:** Specifies the default command to run when a container starts from this image.

**Format:** JSON array format: `["command", "arg1", "arg2"]`

**Note:** There can only be one `CMD` in a Dockerfile. If you have multiple, only the last one takes effect.

### Building Your Image

Now let's build this image!

**Step 1:** Make sure you're in the directory with your `Dockerfile`:

```bash
cd /path/to/your/project
```

**Step 2:** Run the build command:

```bash
docker build -t my-node-app:1.0 .
```

**Breaking down this command:**

- `docker build` = Build an image
- `-t my-node-app:1.0` = Tag (name) the image "my-node-app" with version "1.0"
- `.` = Build context (current directory)

**What you'll see:**

```
[+] Building 12.3s (10/10) FINISHED
 => [1/5] FROM node:18-alpine
 => [2/5] WORKDIR /app
 => [3/5] COPY package*.json ./
 => [4/5] RUN npm install
 => [5/5] COPY . .
 => exporting to image
 => => naming to docker.io/library/my-node-app:1.0
```

Each `=>` represents a layer being built!

**Step 3:** Verify your image was created:

```bash
docker images
```

You should see `my-node-app` listed!

### Running Your Custom Container

Now run a container from your image:

```bash
docker run -d -p 3000:3000 --name my-app my-node-app:1.0
```

Open your browser and go to `http://localhost:3000`. You should see:

**"Hello from Docker!"**

üéâ Congratulations! You just containerized your first application!

## Dockerfile Instructions: The Complete Guide

Let's explore more Dockerfile instructions you'll commonly use.

### FROM: Choosing Your Base Image

```dockerfile
FROM ubuntu:22.04
FROM node:18-alpine
FROM python:3.11-slim
```

**Best practices:**

- Use specific version tags, not `latest`
- Prefer Alpine or slim variants for smaller images
- Choose official images when possible

### RUN: Executing Commands During Build

```dockerfile
RUN apt-get update && apt-get install -y curl
RUN pip install -r requirements.txt
RUN go build -o /app/server
```

**Best practices:**

- Combine related commands with `&&` to reduce layers:

  ```dockerfile
  # Bad (creates 2 layers)
  RUN apt-get update
  RUN apt-get install -y curl

  # Good (creates 1 layer)
  RUN apt-get update && apt-get install -y curl
  ```

### COPY vs ADD: Adding Files

**COPY (recommended for most cases):**

```dockerfile
COPY package.json .
COPY src/ /app/src/
COPY . .
```

**ADD (has extra features):**

```dockerfile
ADD https://example.com/file.tar.gz /tmp/
ADD archive.tar.gz /app/
```

**Difference:**

- `COPY` just copies files
- `ADD` can extract tarballs and download from URLs
- **Use COPY unless you specifically need ADD's features**

### ENV: Setting Environment Variables

```dockerfile
ENV NODE_ENV=production
ENV PORT=3000
ENV DATABASE_URL=postgres://db:5432/myapp
```

**Usage:** Access in your application as environment variables.

### WORKDIR: Setting the Working Directory

```dockerfile
WORKDIR /app
WORKDIR /app/src
```

**Why use it:** Better than `RUN cd /app` because it's persistent across instructions.

### ARG: Build-time Variables

```dockerfile
ARG NODE_VERSION=18
FROM node:${NODE_VERSION}-alpine

ARG BUILD_DATE
LABEL build_date=${BUILD_DATE}
```

**Usage during build:**

```bash
docker build --build-arg NODE_VERSION=20 -t my-app .
```

**Difference from ENV:** `ARG` only exists during build, `ENV` exists at runtime.

### EXPOSE: Documenting Ports

```dockerfile
EXPOSE 3000
EXPOSE 8080
```

**Remember:** This is just documentation. Use `-p` when running to actually publish ports.

### CMD vs ENTRYPOINT: Running Commands

**CMD (default command, can be overridden):**

```dockerfile
CMD ["npm", "start"]
```

Run with override:

```bash
docker run my-app npm run dev  # Uses "npm run dev" instead
```

**ENTRYPOINT (always runs, arguments can be appended):**

```dockerfile
ENTRYPOINT ["python", "app.py"]
```

Run with additional args:

```bash
docker run my-app --debug  # Runs: python app.py --debug
```

**Using both together:**

```dockerfile
ENTRYPOINT ["python", "app.py"]
CMD ["--port", "8000"]
```

Now `CMD` becomes default arguments to `ENTRYPOINT`:

- Default: `python app.py --port 8000`
- Override: `docker run my-app --port 9000` ‚Üí `python app.py --port 9000`

### USER: Running as Non-Root

```dockerfile
# Create a user
RUN addgroup -g 1001 appgroup && \
    adduser -D -u 1001 -G appgroup appuser

# Switch to that user
USER appuser
```

**Security best practice:** Don't run applications as root in containers!

### VOLUME: Declaring Volume Mount Points

```dockerfile
VOLUME /app/data
```

**What it does:** Indicates that this directory should be a volume.

**Note:** Usually better to specify volumes when running the container with `-v`.

## Building Efficient Images: Best Practices

### 1. Use .dockerignore

Create a `.dockerignore` file (like `.gitignore`):

```
node_modules
npm-debug.log
.git
.env
*.md
.DS_Store
```

**Why:** Prevents unnecessary files from being copied into the image, making it smaller and builds faster.

### 2. Order Matters: Leverage Layer Caching

**Bad order (cache busted frequently):**

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY . .                    # ‚Üê Code changes often
RUN npm install             # ‚Üê This runs every time code changes!
CMD ["npm", "start"]
```

**Good order (maximizes caching):**

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./       # ‚Üê Only changes when dependencies change
RUN npm install             # ‚Üê Cached unless package.json changes
COPY . .                    # ‚Üê Code changes, but deps already installed
CMD ["npm", "start"]
```

### 3. Minimize Layers

**Bad (many layers):**

```dockerfile
RUN apt-get update
RUN apt-get install -y curl
RUN apt-get install -y git
RUN apt-get clean
```

**Good (fewer layers):**

```dockerfile
RUN apt-get update && \
    apt-get install -y \
        curl \
        git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
```

### 4. Multi-Stage Builds: Smaller Production Images

This is advanced but incredibly powerful!

**Problem:** Build tools (compilers, dev dependencies) make images huge, but you don't need them in production.

**Solution:** Use multiple `FROM` statements‚Äîbuild in one stage, copy artifacts to a clean final stage.

**Example: Go application**

```dockerfile
# Stage 1: Build
FROM golang:1.21 AS builder
WORKDIR /app
COPY . .
RUN go build -o server

# Stage 2: Runtime
FROM alpine:latest
WORKDIR /app
COPY --from=builder /app/server .
CMD ["./server"]
```

**Result:** Final image only contains the compiled binary, not the entire Go toolchain!

**Size difference:**

- Single stage: ~800MB (includes Go compiler)
- Multi-stage: ~15MB (just Alpine + binary)

### 5. Choose the Right Base Image

**Size comparison for Node.js:**

- `node:18` (Debian): ~900MB
- `node:18-slim` (Minimal Debian): ~200MB
- `node:18-alpine` (Alpine Linux): ~170MB

**Trade-off:** Alpine is smallest but uses `musl` instead of `glibc`, which can cause compatibility issues with some packages.

**Recommendation:** Start with Alpine, fall back to slim if you hit issues.

## Container Registries: Sharing Your Images

Once you've built your image, you need somewhere to store and share it. That's where container registries come in.

### What is a Container Registry?

**Think of it like:**

- **GitHub** for code ‚Üí **Container Registry** for images
- **App Store** for apps ‚Üí **Container Registry** for containers

**Purpose:**

1. **Storage:** Centralized place to store images
2. **Distribution:** Share images with your team or the world
3. **Versioning:** Keep multiple versions of images
4. **Security:** Private registries for proprietary images

### Popular Container Registries

**1. Docker Hub (docker.io)**

- ‚úÖ Free public repositories
- ‚úÖ Largest registry (100,000+ images)
- ‚úÖ Great for open source
- üí∞ Private repos require paid plan

**2. GitHub Container Registry (ghcr.io)**

- ‚úÖ Free private repos
- ‚úÖ Integrates with GitHub Actions
- ‚úÖ Good for personal/team projects

**3. Amazon ECR**

- ‚úÖ Deep AWS integration
- ‚úÖ Excellent for production
- üí∞ Pay for storage and bandwidth

**4. Google Container Registry (GCR)**

- ‚úÖ Deep GCP integration
- ‚úÖ Vulnerability scanning

**5. Azure Container Registry (ACR)**

- ‚úÖ Deep Azure integration
- ‚úÖ Geo-replication

### Pushing to Docker Hub

Let's walk through publishing your image to Docker Hub.

**Step 1:** Create a Docker Hub account at [hub.docker.com](https://hub.docker.com)

**Step 2:** Log in from the command line:

```bash
docker login
```

Enter your username and password.

**Step 3:** Tag your image with your Docker Hub username:

```bash
docker tag my-node-app:1.0 yourusername/my-node-app:1.0
```

**Format:** `docker tag local-name:tag username/repository:tag`

**Step 4:** Push to Docker Hub:

```bash
docker push yourusername/my-node-app:1.0
```

**Step 5:** Verify on Docker Hub‚Äîyou should see your image in your repositories!

**Now anyone can run your image:**

```bash
docker run yourusername/my-node-app:1.0
```

### Image Naming and Tagging Strategy

**Full image name format:**

```
registry/namespace/repository:tag
```

**Examples:**

```
docker.io/library/nginx:latest
ghcr.io/mycompany/web-app:v1.2.3
myregistry.azurecr.io/backend:2024-01-15
```

**Tagging best practices:**

**1. Use semantic versioning:**

```bash
docker tag my-app:latest my-app:1.0.0
docker tag my-app:latest my-app:1.0
docker tag my-app:latest my-app:1
```

**2. Tag by environment:**

```bash
my-app:dev
my-app:staging
my-app:production
```

**3. Include build metadata:**

```bash
my-app:v1.2.3-alpine
my-app:v1.2.3-debian
my-app:commit-abc123f
my-app:2024-01-15
```

**4. Avoid using `latest` in production:**

- It's ambiguous (which version is "latest"?)
- Makes rollbacks harder
- Harder to reproduce issues

## Running Containers: Advanced Options

Now that you're building images, let's explore more ways to run them effectively.

### Environment Variables: Configuration at Runtime

**Pass variables with `-e`:**

```bash
docker run -e DATABASE_URL=postgres://localhost:5432/mydb \
           -e NODE_ENV=production \
           my-app
```

**Use an env file:**

Create `.env`:

```
DATABASE_URL=postgres://localhost:5432/mydb
NODE_ENV=production
API_KEY=secret123
```

Run with:

```bash
docker run --env-file .env my-app
```

### Resource Limits: Controlling CPU and Memory

**Limit memory:**

```bash
docker run -m 512m my-app  # Max 512MB RAM
```

**Limit CPU:**

```bash
docker run --cpus=1.5 my-app  # Max 1.5 CPU cores
```

**Why limit resources:** Prevents one container from hogging all system resources.

### Restart Policies: Automatic Recovery

```bash
docker run --restart=always my-app       # Always restart
docker run --restart=unless-stopped my-app  # Restart unless manually stopped
docker run --restart=on-failure my-app   # Only restart if exit code != 0
```

**Use case:** For production services, use `--restart=unless-stopped` to survive system reboots.

### Networking: Connecting Containers

**Default:** Each container is isolated.

**Create a network:**

```bash
docker network create my-network
```

**Run containers on the same network:**

```bash
docker run -d --name postgres --network my-network postgres:15
docker run -d --name api --network my-network my-api-image
```

**Now `api` can connect to `postgres` using the hostname `postgres`!**

**Example connection string:**

```
postgres://postgres:5432/mydb
```

Docker's internal DNS resolves `postgres` to the container's IP.

### Health Checks: Monitoring Container Health

Add to your Dockerfile:

```dockerfile
HEALTHCHECK --interval=30s --timeout=3s --retries=3 \
  CMD curl -f http://localhost:3000/health || exit 1
```

**What it does:**

- Every 30 seconds, checks if `http://localhost:3000/health` responds
- If it fails 3 times in a row, marks container as "unhealthy"

**Check health status:**

```bash
docker ps  # Shows health status
docker inspect my-app | grep Health
```

## Real-World Example: Full-Stack Application

Let's put it all together with a realistic example.

### Project Structure

```
my-fullstack-app/
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îî‚îÄ‚îÄ docker-compose.yml
```

### Backend Dockerfile

```dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .

FROM node:18-alpine

RUN addgroup -g 1001 appgroup && \
    adduser -D -u 1001 -G appgroup appuser

WORKDIR /app
COPY --from=builder /app .

USER appuser
EXPOSE 4000

HEALTHCHECK --interval=30s --timeout=3s \
  CMD node healthcheck.js || exit 1

CMD ["node", "server.js"]
```

### Frontend Dockerfile

```dockerfile
# Build stage
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci

COPY . .
RUN npm run build

# Production stage
FROM nginx:alpine

COPY --from=builder /app/build /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

### Docker Compose (Bonus!)

Instead of running containers manually, use Docker Compose:

```yaml
version: '3.8'

services:
  database:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: secretpass
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - app-network

  backend:
    build: ./backend
    ports:
      - '4000:4000'
    environment:
      DATABASE_URL: postgres://postgres:secretpass@database:5432/mydb
    depends_on:
      - database
    networks:
      - app-network

  frontend:
    build: ./frontend
    ports:
      - '3000:80'
    depends_on:
      - backend
    networks:
      - app-network

networks:
  app-network:

volumes:
  postgres-data:
```

**Run everything with one command:**

```bash
docker-compose up
```

**Stop everything:**

```bash
docker-compose down
```

## Troubleshooting Common Build Issues

### Issue 1: "Cannot find module"

**Symptom:** App works locally but crashes in container.

**Cause:** Forgot to copy dependencies or run install.

**Fix:** Ensure your Dockerfile has:

```dockerfile
COPY package*.json ./
RUN npm install
```

### Issue 2: Huge Image Size

**Symptom:** Image is multiple gigabytes.

**Causes:**

- Using full base image instead of slim/alpine
- Including unnecessary files
- Not using multi-stage builds

**Fixes:**

- Use `.dockerignore`
- Choose smaller base image
- Implement multi-stage build

### Issue 3: Slow Builds

**Symptom:** Every build takes forever.

**Cause:** Poor layer caching strategy.

**Fix:** Order Dockerfile to put stable things first:

```dockerfile
# Do this (stable to changing)
FROM ...
COPY package.json .
RUN install-deps
COPY source-code .
```

### Issue 4: "Permission Denied"

**Symptom:** Can't write files in container.

**Cause:** Running as non-root user without proper permissions.

**Fix:**

```dockerfile
RUN mkdir /app/data && chown appuser:appgroup /app/data
USER appuser
```

## Conclusion: You're Now a Docker Image Builder!

You've learned an incredible amount in this article:

‚úÖ How Docker images are structured with layers  
‚úÖ Writing Dockerfiles with all essential instructions  
‚úÖ Building custom images for your applications  
‚úÖ Optimizing images for size and build speed  
‚úÖ Using multi-stage builds for production  
‚úÖ Pushing images to container registries  
‚úÖ Running containers with advanced configuration  
‚úÖ Connecting multiple containers together

You now have the skills to take any application‚Äîwhether it's Node.js, Python, Go, or any other technology‚Äîand containerize it professionally.

## Frequently Asked Questions

**Q: Should I build images on my laptop or use CI/CD?**

A: Both! Build locally during development to test. Use CI/CD (GitHub Actions, GitLab CI, etc.) to build production images automatically on every commit. This ensures consistency and allows for automated testing.

**Q: How do I update an application running in production?**

A: The typical flow:

1. Make code changes
2. Build a new image with a new tag (e.g., `v1.2.1`)
3. Push to registry
4. Update your deployment to use the new tag
5. Kubernetes/Docker Swarm will pull the new image and replace old containers

This allows for zero-downtime deployments!

**Q: What's the difference between `docker build` and `docker compose build`?**

A:

- `docker build` builds a single image from a Dockerfile
- `docker compose build` builds all images defined in your `docker-compose.yml`

Docker Compose is a tool for multi-container applications. It's incredibly useful for development environments!

## Take the Next Step

Here's how to practice these skills:

1. **Containerize a personal project** you've built
2. **Experiment with multi-stage builds** to see the size reduction
3. **Push an image to Docker Hub** and share it with a friend
4. **Try different base images** (Debian vs Alpine) and compare sizes
5. **Set up Docker Compose** for a multi-container app

**Challenge:** Build a Dockerfile for a Python Flask app with PostgreSQL database. Create a multi-stage build and get the final image under 100MB!

---

**Continue Learning:**

- Part 1: [Understanding Docker: A Beginner's Guide to Containers](./docker-introduction-history.md)
- Part 2: [Getting Started with Docker](./docker-getting-started-installation.md)
- Part 4: [Docker Security, Development Workflow & Deployment](#) (Coming soon!)

**Watch the Full Course:** [Complete Docker Course - From BEGINNER to PRO! (Learn Containers)](https://www.youtube.com/watch?v=RqTEHSBrYFw)

**Timestamp for this article:** 1:13:00 - 3:02:36

Ready to deploy your containers to production? Let us know in the comments what you'd like to learn next!